{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Machine Learning with scikit-learn\n",
    "\n",
    "## About me - Ted Petrou\n",
    "\n",
    "* Author \n",
    "    * Pandas Cookbook\n",
    "    * [Exercise Python][1]\n",
    "    * [Master Data Analysis with Python][2]\n",
    "    * [Master Machine Learning with Python][3]\n",
    "* Founder of [Dunder Data][4]\n",
    "* Author of [Dexplo and Dexplot][5]\n",
    "\n",
    "\n",
    "[1]: https://www.dunderdata.com/exercise-python\n",
    "[2]: https://www.dunderdata.com/master-data-analysis-with-python\n",
    "[3]: https://www.dunderdata.com/master-machine-learning-with-python\n",
    "[4]: https://dunderdata.com\n",
    "[5]: https://github.com/dexplo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics\n",
    "\n",
    "* What is machine learning?\n",
    "* Exploratory data analysis\n",
    "* Modeling by hand\n",
    "* The scikit-learn Estimator\n",
    "* Different regression estimators\n",
    "* Cross validation\n",
    "* Grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning vs Machine Learning\n",
    "\n",
    "### What is Learning?\n",
    "Learning is the ability to improve at a **task**. Learning is done by animals, humans, and some machines.\n",
    "\n",
    "### What is a task?\n",
    "A task is a clearly defined piece of work.\n",
    "\n",
    "### Measuring task performance\n",
    "Learning happens when the person or machine improves its performance at completing the task. \n",
    "\n",
    "### What is Machine Learning?\n",
    "Machine learning is often defined as the ability of a machine to learn (to improve on a specific task) without being explicitly programmed to do so.\n",
    "\n",
    "### What is \"not explicitly programmed\"?\n",
    "Not updated by a human\n",
    "\n",
    "### The two types of machine learning\n",
    "* **supervised** - have labels (ground truth)\n",
    "* **unsupervised** - no labels\n",
    "\n",
    "### Regression vs Classification\n",
    "* **regression** - continuous value labels\n",
    "* **classification** - discrete labels\n",
    "\n",
    "\n",
    "## Terminology\n",
    "\n",
    "![][1]\n",
    "\n",
    "\n",
    "## Assessing task performance\n",
    "Objectively quantifiable measure of performance\n",
    "\n",
    "### Assessing regression task performance\n",
    "Minimize error\n",
    "\n",
    "[1]: images/terminology.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ames Housing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* [Famous beginners Kaggle competition][0] compiled by professor Dean De Cock from Ames, Iowa from 2006 - 2010\n",
    "* Original dataset has 79 features and 1460 samples\n",
    "* For simplicity, we will only look at 8 features\n",
    "* Predict sale price\n",
    "* Evaluation metric - minimize squared error\n",
    "\n",
    "[0]: https://www.kaggle.com/c/house-prices-advanced-regression-techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = pd.read_csv('data/housing_sample.csv')\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data dictionary\n",
    "\n",
    "Open up the [data dictionary for these columns][0] in a separate tab and read through the column descriptions.\n",
    "\n",
    "[0]: data/housing_sample_data_dictionary.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis\n",
    "\n",
    "Gain an understanding of the data without the use of formal modeling.\n",
    "\n",
    "* Relies heavily on visualization\n",
    "* Begin with univariate analysis\n",
    "    * Categorical features (limited, known, discrete values)\n",
    "        * Frequency of occurrence of each value\n",
    "    * Continuous features ()\n",
    "        * Summary statistics (min, max, mean, median, std, etc...)\n",
    "        * Distribution plots (boxplot, histogram, KDE)\n",
    "* Relationship with target\n",
    "    * Compare each feature with target\n",
    "    * Categorical features\n",
    "        * Mean sale price by category\n",
    "    * Continuous features\n",
    "        * Scatterplot of feature vs sale price\n",
    "        * Bin feature and take mean per bin\n",
    "        * Correlation vs sale price\n",
    "* Multivariate analysis\n",
    "    * All categorical features - crosstabulation, pivot table with sales price\n",
    "    * All continuous - scatterplots\n",
    "    * Mix of categorical and continuous - distribution plot by group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic information on data\n",
    "\n",
    "* Number of features and observations\n",
    "* Data type of each column\n",
    "* Number of missing values per column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate analysis\n",
    "\n",
    "Begin with categorical columns\n",
    "\n",
    "* Neighborhood\n",
    "* Exterior1st\n",
    "* BedroomAbvGr\n",
    "* FullBath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['Neighborhood', 'Exterior1st', 'BedroomAbvGr', 'FullBath']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing['Neighborhood'].value_counts().plot(kind='bar', figsize=(10, 4));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing['Exterior1st'].value_counts().plot(kind='bar', figsize=(10, 4));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing['BedroomAbvGr'].value_counts().sort_index().plot(kind='bar', figsize=(10, 4));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing['FullBath'].value_counts().sort_index().plot(kind='bar', figsize=(10, 4));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous columns\n",
    "\n",
    "\n",
    "* YearBuilt\t\n",
    "* LotFrontage\t\n",
    "* GrLivArea\t\n",
    "* GarageArea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_cols = ['YearBuilt', 'LotFrontage', 'GrLivArea', 'GarageArea']\n",
    "fig, axes = plt.subplots(4, 2, figsize=(10, 12))\n",
    "for i, col in enumerate(continuous_cols):\n",
    "    housing[col].plot(kind='box', vert=False, ax=axes[i, 0]);\n",
    "    axes[i, 1] = housing[col].plot(kind='hist', ax=axes[i, 1], title=col);\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship with target\n",
    "\n",
    "Categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8), sharey=True)\n",
    "for col, ax in zip(categorical_cols, axes.flatten()):\n",
    "    housing.groupby(col)['SalePrice'].mean().plot(kind='bar', ax=ax)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8), sharey=True)\n",
    "for col, ax in zip(continuous_cols, axes.flatten()):\n",
    "    housing.plot(x=col, y = 'SalePrice', kind='scatter', ax=ax)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.crosstab(index=housing['Neighborhood'], columns=housing['FullBath'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average sale price for bedroom and bathroom combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.pivot_table(index='BedroomAbvGr', columns='FullBath', \n",
    "                    values='SalePrice', aggfunc='mean').round(-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use seaborn to plot regression line of above ground living area vs sale price. Make a separate line for each unique bedroom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.lmplot(x='GrLivArea', y='SalePrice', data=housing, hue='BedroomAbvGr', aspect=2, ci=None) \\\n",
    "   .set(xlim=(500, 3000), ylim=(0, 500000));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr_bins = pd.cut(housing['GrLivArea'], range(500, 4000, 500))\n",
    "housing.pivot_table(index='Neighborhood', columns=gr_bins, \n",
    "                    values='SalePrice', aggfunc='mean') // 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.corrwith(housing['SalePrice']).round(2).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a simple model without scikit-learn\n",
    "\n",
    "scikit-learn automates the learning process for you. Instead of beginning with it, I recommend building a simple model without it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are we predicting?\n",
    "In this problem, we want to predict the final sale price of the house.\n",
    "\n",
    "### Assign target variable to `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = housing['SalePrice']\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model for regression\n",
    "\n",
    "One of the simplest models we can build for regression problems is guessing the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = y.mean()\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess model performance\n",
    "\n",
    "For each model we build, we will assess its performance. A popular measure of performance for regression problems is the Sum of Squared Error or SSE. This calculation involves three steps:\n",
    "\n",
    "* Calculate error for each observation - actual - mean\n",
    "* Square each error\n",
    "* Sum the errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Calculate error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = y - y_pred\n",
    "error.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Square errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_squared = error ** 2\n",
    "error_squared.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Sum errors\n",
    "\n",
    "Get a single metric to objectively quantify our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_sse = error_squared.sum()\n",
    "model_1_sse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 - Use average neighborhood sale price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = housing.groupby('Neighborhood')['SalePrice'].mean()\n",
    "model_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_2.loc[housing['Neighborhood']].values.round(-3)\n",
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate SSE in one step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_sse = ((y - y_pred) ** 2).sum()\n",
    "model_2_sse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing['GrLivArea'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr_bins = pd.cut(housing['GrLivArea'], range(0, 6500, 500))\n",
    "model_3 = housing.groupby(['Neighborhood', gr_bins])['SalePrice'].mean()\n",
    "model_3.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = housing[['Neighborhood', 'GrLivArea']].itertuples(index=False)\n",
    "y_pred = model_3[X].values.round(-3)\n",
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3_sse = ((y - y_pred) ** 2).sum()\n",
    "model_3_sse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot errors for simple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = {'model_1_simple_avg': model_1_sse,\n",
    "          'model_2_neigh_avg': model_2_sse, \n",
    "          'model_3_neigh_area_avg': model_3_sse}\n",
    "pd.Series(errors).plot(kind='bar', figsize=(12, 5), rot=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning with scikit-learn\n",
    "\n",
    "* scikit-learn provides many regression models to learn from data\n",
    "* The goal of each model is to minimize the squared error \n",
    "\n",
    "### Linear Regression\n",
    "\n",
    "Linear regression is one of the simpler models available in scikit-learn. When learning with a single feature, it's called **simple linear regression**. The form of the model is as follows:\n",
    "\n",
    "$$\\hat{y} = w_{0} + w_{1}x_{1}$$\n",
    "\n",
    "Where $\\hat{y}$ (pronounced y-hat) is the predicted value, $x_1$ is the feature value, and $w_0$ and $w_1$ are the parameters. This is the equation or a line where $w_0$ is the y-intercept and $w_1$ is the slope. \n",
    "\n",
    "### Goal of linear regression\n",
    "\n",
    "The goal of all models is to learn from data. With linear regression, we would like to find the value of the parameters ($w_0$ and $w_1$) that minimize the SSE. Once the values for the parameters are found then the learning is complete.\n",
    "\n",
    "### Select a single feature to learn from\n",
    "\n",
    "We'll select above ground living area (`GrLivArea`) as our single feature for our first linear regression model. Let's plot the relationship between it and the sale price as a scatterplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(x='GrLivArea', y='SalePrice', kind='scatter', figsize=(12, 6));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose values of parameters by hand\n",
    "\n",
    "Before using scikit-learn, let's choose a couple different combinations of $w_0$ and $w_1$ and plot the lines and calculate error. First we create a function that makes the predictions for us given the parameters and the feature values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_predict(w0, w1, x):\n",
    "    return w0 + w1 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this function to plot two models with different parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = housing.plot(x='GrLivArea', y='SalePrice', kind='scatter', figsize=(12, 6));\n",
    "\n",
    "# model 1 intercept of 50,000 and slope of 75\n",
    "x = np.array([[0], [5000]])\n",
    "w0, w1 = 50_000, 75\n",
    "ax.plot(x, lr_predict(w0, w1, x), color='red', lw=3, label=f'$w_0={w0:,}$\\n$w_1={w1}$')\n",
    "\n",
    "# model 2 intercept of 0 and slope of 125\n",
    "w0, w1 = 0, 125\n",
    "ax.plot(x, lr_predict(w0, w1, x), color='green', lw=3, label=f'$w_0={w0}$\\n$w_1={w1}$')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate errors for each model\n",
    "\n",
    "Use root mean squared error (RMSE) instead. This is a very similar metric, but is relative and not absolute. Much smaller and easier to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr_predict(50_000, 75, housing['GrLivArea'])\n",
    "y_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_lr = np.sqrt(((y - y_pred) ** 2).mean()).round()\n",
    "model_1_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr_predict(0, 125, housing['GrLivArea'])\n",
    "model_2_lr = np.sqrt(((y - y_pred) ** 2).mean()).round()\n",
    "model_2_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model two has lowest error and is best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The scikit-learn Estimator\n",
    "\n",
    "We now turn to scikit-learn to automate the learning process. scikit-learn will quickly find us the best combination of linear regression parameter values that minimize the squared error.\n",
    "\n",
    "scikit-learn uses the term **Estimator** to refer to any object that learns from data. There are several types of estimators that scikit-learn has built for us. Not all estimators are machine learning models, but all of them **learn from data**. Here are the main types of estimators:\n",
    "\n",
    "* Regressors - Supervised learning with continuous target\n",
    "* Classifiers - Supervised learning with categorical target\n",
    "* Clusterers - Unsupervised learning\n",
    "* Transformers - Transform the input/output data\n",
    "* Meta-estimators - Learn from other estimators\n",
    "\n",
    "### Understanding the scikit-learn API\n",
    "\n",
    "It is instructive to take a glimpse of the scikit-learn API before using it. I like to analogize the API to a house where each of the rooms contains the useful objects for machine learning. Take a look at the scikit-learn 'house' below:\n",
    "\n",
    "![][1]\n",
    "\n",
    "[1]: images/scikit_house.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression in scikit-learn\n",
    "\n",
    "While the above models look reasonable, they do not minimize the squared error. scikit-learn uses an algorithm that guarantees to find the combination of parameters that minimize the SSE.\n",
    "\n",
    "### Data requirements for scikit-learn\n",
    "\n",
    "In order to use scikit-learn, the data must be provided in a particular form. Here is how you need to provide your data:\n",
    "\n",
    "* The input data must be two-dimensional\n",
    "* The output data can be one-dimensional\n",
    "* There can be no missing values\n",
    "* The input data cannot contain strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select feature as a DataFrame and Target as a Series\n",
    "\n",
    "scikit-learn requires our input data to be two dimensional, even if we are using just a single feature as our input. Below, we select our feature as a DataFrame using a single-item list. By convention, scikit-learn uses the variable names `X` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = housing[['GrLivArea']]\n",
    "y = housing['SalePrice']\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Three-step process for all Estimators - Import, Instantiate, Fit\n",
    "\n",
    "All scikit-learn estimators follow the same three step process:\n",
    "\n",
    "* Import - find model in scikit-learn 'house'\n",
    "* Instantiate - create a single instance of the model\n",
    "* Fit - learn from data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Instantiate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the model\n",
    "\n",
    "After calling the `fit` method, the model is trained. We can find the optimal parameter values by accessing the `intercept_` and `coef_` attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions with the `predict` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions on all input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = housing.plot(x='GrLivArea', y='SalePrice', kind='scatter', figsize=(12, 6))\n",
    "\n",
    "# model 1 intercept of 50,000 and slope of 75\n",
    "x = np.array([[0], [5000]])\n",
    "w0, w1 = 50_000, 75\n",
    "ax.plot(x, lr_predict(w0, w1, x), color='red', lw=3, label=f'$w_0={w0:,}$\\n$w_1={w1}$')\n",
    "\n",
    "# model 2 intercept of 0 and slope of 125\n",
    "w0, w1 = 0, 125\n",
    "ax.plot(x, lr_predict(w0, w1, x), color='green', lw=3, label=f'$w_0={w0}$\\n$w_1={w1}$')\n",
    "\n",
    "# LinearRegression in scikit-learn\n",
    "y_pred = lr.predict(x)\n",
    "w0 = lr.intercept_.round(-3)\n",
    "w1 = lr.coef_[0]\n",
    "ax.plot(x, y_pred, color='black', lw=5, ls='--', label=f'$w_0={w0:,.0f}$\\n$w_1={w1:.0f}$')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing model performance - $R^2$\n",
    "\n",
    "scikit-learn uses, $R^2$, a slightly different metric as its default for model performance. Here is the procedure for calculating $R^2$\n",
    "\n",
    "* Find the SSE of the model\n",
    "* Find the SSE when guessing the mean\n",
    "* $R^2$ is the percentage decrease in the SSE when guessing the mean versus the SSE of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate $R^2$ by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_model = lr.predict(X)\n",
    "y_pred_mean = y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sse_model = ((y - y_pred_model) ** 2).sum()\n",
    "sse_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sse_mean = ((y - y_pred_mean) ** 2).sum()\n",
    "sse_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = 1 - sse_model / sse_mean\n",
    "r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use score method to calculate $R^2$\n",
    "\n",
    "All regression estimators have a `score` method that returns the $R^2$. Let's verify that it's the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing $R^2$\n",
    "\n",
    "Best possible score is 1.\n",
    "\n",
    "![][0]\n",
    "\n",
    "[0]: images/r2_matplotlib.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $R^2$ - percent variance explained\n",
    "\n",
    "The SSE when guessing the mean is also the definition of the statistical variance (multiplied by the number of observations). Therefore, the SSE of the model is the percentage decrease in this variance. The variance is the natural variation of the target. We are interested in understanding why it varies. Why isn't the target the same for every input? We use a model to explain this variance. We'd like to explain all of the variance. The metric $R^2$ informs us of how much this natural inherent variance has been removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Instantiate a new `LinearRegression` estimator and use a different feature to learn from. Output the parameter values and the $R^2$ score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression\n",
    "\n",
    "Use more than one predictor variable. New model:\n",
    "\n",
    "$$\\hat{y} = w_{0} + w_{1}x_{1} + w_{2}x_{2} + ... + w_{p}x_{p}$$\n",
    "\n",
    "Where `p` is the number of predictor variables (features).\n",
    "\n",
    "### Select multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['GrLivArea', 'GarageArea', 'FullBath']\n",
    "X = housing[cols]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model again - no need to import\n",
    "\n",
    "Since we already imported the `LinearRegression` class, we can straight to step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit with new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get learned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model interpretation\n",
    "\n",
    "For every one unit increase in the feature, a corresponding increase of the parameter value is expected in the target (many assumptions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other ML models - Back at the scikit-learn house\n",
    "\n",
    "![][0]\n",
    "\n",
    "[0]: images/scikit_house.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors\n",
    "\n",
    "K-Nearest neighbors (KNN) provides a completely different approach to learning from data. KNN finds the most similar observations to the one being predicted. It then averages the target variable for all of these similar neighbors to make a prediction. You must supply it a value, `k`, for the number of neighbors you would like to find.\n",
    "\n",
    "### Visualizing KNN\n",
    "\n",
    "The below function accepts a value for an above ground living area, garage area, and number of neighbors. It then calculates the distance between the provided values and all other values in the dataset. The k nearest neighbors are found based on this distance calculation. The average sales price of these five neighbors is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_neighbors(sq_foot, sq_foot_garage, k):\n",
    "    df = housing[['GrLivArea', 'GarageArea', 'SalePrice']].copy()\n",
    "    df['distance'] = np.sqrt((df['GrLivArea'] - sq_foot) ** 2 + (df['GarageArea'] - sq_foot_garage) ** 2)\n",
    "    df_neighbors = df.nsmallest(k, 'distance')\n",
    "    mean = df_neighbors['SalePrice'].mean()\n",
    "    ax = df.plot(x='GrLivArea', y='GarageArea', kind='scatter', figsize=(14, 6))\n",
    "    df_neighbors.plot(x='GrLivArea', y='GarageArea', kind='scatter', color='red', ax=ax, label=f'{k} neighbors')\n",
    "    ax.scatter([sq_foot], [sq_foot_garage], color='green', s=40, label='Predicted Value')\n",
    "    ax.set_title(f'{k} nearest neighbors of {sq_foot} square feet and {sq_foot_garage} '\n",
    "                 f'basement square feet predict sale price of {mean}')\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_neighbors(3500, 700, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN in scikit-learn\n",
    "\n",
    "The `KNeighborsRegressor` estimator from the `neighbors` module is available in scikit-learn to do KNN. Just like all estimators, it follows the same three-step process - import, instantiate, fit. When we instantiate it, we set the number of neighbors with the `n_neighbors` parameter. All three steps are completed in a single cell. We select the same features as the plot above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['GrLivArea', 'GarageArea']\n",
    "X = housing[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score\n",
    "\n",
    "By default, all scikit-learn regression estimators use $R^2$ as their evaluation metric in the `score` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "Decision trees are one of the more interpretable models (when not so deep) and are composed of:\n",
    "\n",
    "* Nodes - where a binary decision is made\n",
    "* Branches - lead you to the next node (decision)\n",
    "* Leaves - bottom of the tree, where prediction is\n",
    "\n",
    "![][0]\n",
    "\n",
    "\n",
    "\n",
    "[0]: images/tree.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision trees in scikit-learn\n",
    "\n",
    "The `DecisionTreeRegressor` estimator from the `tree` module learns a decision tree in scikit-learn. Let's use a different set of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['BedroomAbvGr', 'FullBath']\n",
    "X = housing[cols]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the same three step process. Set the maximum depth of the tree with the `max_depth` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions and score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the decision tree\n",
    "\n",
    "You'll need to install the graphviz library with `conda install graphviz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "from sklearn.tree import export_graphviz\n",
    "dot_data = export_graphviz(dtr, out_file=None, feature_names=cols, precision=1,\n",
    "                           filled=True, rounded=True,  special_characters=True)  \n",
    "graph = graphviz.Source(dot_data, format='png')  \n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get nearly a perfect score\n",
    "\n",
    "Decision trees are very flexible models and can provide a nearly perfect fit to the data it is trained on. Let's build a new decision on a different set of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['GrLivArea', 'GarageArea', 'FullBath']\n",
    "X = housing[cols]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we do not set the `max_depth` parameter, then there will be no limit to how deep the tree can grow. It will grow until only a single observation remains in each node. With no limit to it's depth, we get nearly a perfect score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "\n",
    "Thus far, we've evaluated ourselves on the data that the model was trained on. Instead, we must evaluate ourselves on unseen data. One procedure for proper evaluation is cross validation.\n",
    "\n",
    "* Divide data into `k` equal partitions\n",
    "* Train a model on `k - 1` partitions\n",
    "* Evaluate the model on the 1 partition not used during training\n",
    "* Record the score on that 1 partition\n",
    "* Repeat process using a different partition for evaluation\n",
    "* Continue until all partitions have been used for evaluation\n",
    "\n",
    "Many different flavors of cross validation. This procedure is called K-fold cross validation where the 'fold' is a partition of the data and 'k' is the number of these partitions. This value is usually between 5 and 10.\n",
    "\n",
    "![][0]\n",
    "\n",
    "[0]: images/kfold.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation in scikit-learn\n",
    "\n",
    "The `cross_val_score` function automates the entire procedure for us. Pass it the instantiated estimator, input and output data and set, and the number of folds with the `cv` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['GrLivArea', 'GarageArea', 'FullBath']\n",
    "X = housing[cols]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation approximates our future performance\n",
    "\n",
    "We want to know how well our model performs in the future. Cross validation is a procedure for approximating this future performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune Hyperparameters with grid-search\n",
    "\n",
    "All models have hyperparameters that change the behavior of the model. These are set during instantiation in step 2. Hyperparameters are NOT learned during training. They are set by you. You choose what they are. Depending on the model, these hyperparameters can drastically change the performance of the model. Decision trees are heavily influenced by their hyperparameter values. \n",
    "\n",
    "### Using the `GridSearchCV` metaestimator\n",
    "\n",
    "Instead of manually iterating over different combinations of hyperparameters, scikit-learn provides the meta-estimator `GridSearchCV` to automate the process for us. To use it, you must create a grid using a dictionary mapping the hyperparameter name as a string to the possible values (as a list or array)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same three-step process exists for meta-estimators. We import it and instantiate it by passing it the instantiated model, the grid, and the number of folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See all results at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(gs.cv_results_)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.pivot(index='param_max_depth', columns='param_min_samples_leaf', \n",
    "                 values='mean_test_score').round(3).style.background_gradient('coolwarm', axis=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
